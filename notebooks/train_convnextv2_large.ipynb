{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13583632,"sourceType":"datasetVersion","datasetId":8619147}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, gc, random, math\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom tqdm import tqdm\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, datasets\nfrom torchvision import transforms\nfrom torchvision.transforms import ColorJitter, RandomPerspective, RandAugment\n\nimport timm\nfrom timm.loss import LabelSmoothingCrossEntropy\ntry:\n    from timm.utils import ModelEmaV2\nexcept:\n    ModelEmaV2 = None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-07T01:01:23.828603Z","iopub.execute_input":"2025-11-07T01:01:23.828826Z","execution_failed":"2025-11-07T01:04:04.079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Config\n\nTRAIN_DIR = \"/kaggle/input/action-dm-dataset/action-dm-dataset_2/train\"\nTEST_DIR  = \"/kaggle/input/action-dm-dataset/action-dm-dataset_2/test/test\"\nMODEL_DIR_TEACHER = \"models_teacher\"\nMODEL_DIR_STUDENT = \"models_student\"\nPSEUDO_CSV_PATH = \"pseudo_labels.csv\"\nOUT_FILE = \"submission.csv\"\n\nos.makedirs(MODEL_DIR_TEACHER, exist_ok=True)\nos.makedirs(MODEL_DIR_STUDENT, exist_ok=True)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\ntorch.backends.cudnn.benchmark = True\n\nBATCH_SIZE = 8\nIMG_SIZE = 384\nBACKBONE = \"convnextv2_large.fcmae_ft_in22k_in1k\"\nN_FOLDS_TEACHER = 3\nEPOCHS_TEACHER = 15\nN_FOLDS_STUDENT = 3\nEPOCHS_STUDENT = 12\nCONFIDENCE_THRESHOLD = 0.87\nACCUMULATION_STEPS = 2\n\nprint(\"DEVICE:\", DEVICE)\nprint(\"BACKBONE:\", BACKBONE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Transforms\n\ntrain_tfms = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomApply([ColorJitter(0.2, 0.2, 0.2, 0.1)], p=0.5),\n    transforms.RandomApply([RandomPerspective(distortion_scale=0.2, p=0.5)], p=0.3),\n    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.3),\n    RandAugment(num_ops=2, magnitude=9),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225]),\n    transforms.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3))\n])\n\nval_test_tfms = transforms.Compose([\n    transforms.Resize(int(IMG_SIZE * 1.14)),\n    transforms.CenterCrop(IMG_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406],\n                         [0.229, 0.224, 0.225])\n])\n\ntta_tfms = [\n    val_test_tfms,\n    transforms.Compose([\n        transforms.Resize(int(IMG_SIZE * 1.14)),\n        transforms.CenterCrop(IMG_SIZE),\n        transforms.RandomHorizontalFlip(p=1.0),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ]),\n    transforms.Compose([\n        transforms.Resize(int(IMG_SIZE * 1.14)),\n        transforms.CenterCrop(IMG_SIZE),\n        transforms.RandomRotation(10),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])\n    ])\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset wrapper\n\nclass SimpleImageDataset(Dataset):\n    def __init__(self, paths, labels, transform=None, source_flags=None):\n        self.paths = paths\n        self.labels = labels\n        self.transform = transform\n        self.source_flags = source_flags if source_flags is not None else [0]*len(paths)  # 0 = real, 1 = pseudo\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, idx):\n        p = self.paths[idx]\n        y = int(self.labels[idx])\n        src = int(self.source_flags[idx])\n        img = Image.open(p).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        return img, y, src","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model factory\n\ndef get_model(backbone_name, n_classes, pretrained=True):\n    print(\"Loading model:\", backbone_name)\n    model = timm.create_model(backbone_name, pretrained=pretrained, num_classes=n_classes)\n    # grad checkpointing if available\n    try:\n        if hasattr(model, \"set_grad_checkpointing\"):\n            model.set_grad_checkpointing(True)\n            print(\" - grad checkpointing enabled\")\n    except Exception:\n        pass\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MixUp / CutMix utils\n\ndef rand_bbox(size, lam):\n    # size: (B, C, H, W)\n    H = size[2]\n    W = size[3]\n    cut_rat = math.sqrt(max(0.0, 1.0 - lam))\n    cut_w = int(W * cut_rat)\n    cut_h = int(H * cut_rat)\n    cx = np.random.randint(0, W)\n    cy = np.random.randint(0, H)\n    x1 = int(np.clip(cx - cut_w // 2, 0, W - 1))\n    y1 = int(np.clip(cy - cut_h // 2, 0, H - 1))\n    x2 = int(np.clip(cx + cut_w // 2, 0, W - 1))\n    y2 = int(np.clip(cy + cut_h // 2, 0, H - 1))\n    # ensure valid box\n    if x2 <= x1:\n        x2 = min(W - 1, x1 + 1)\n    if y2 <= y1:\n        y2 = min(H - 1, y1 + 1)\n    return x1, y1, x2, y2\n\ndef cutmix_data(x, y, alpha=1.0):\n    if alpha <= 0:\n        return x, y, y, 1.0\n    lam = np.random.beta(alpha, alpha)\n    index = torch.randperm(x.size(0)).to(x.device)\n    x1, y1, x2, y2 = rand_bbox(x.size(), lam)\n    # apply exchange\n    x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]\n    # recompute lam as pixel ratio\n    lam = 1.0 - ((x2 - x1) * (y2 - y1) / (x.size(-1) * x.size(-2)))\n    return x, y, y[index], lam\n\ndef mixup_data(x, y, alpha=1.0):\n    if alpha <= 0:\n        return x, y, y, 1.0\n    lam = np.random.beta(alpha, alpha)\n    index = torch.randperm(x.size(0)).to(x.device)\n    mixed_x = lam * x + (1. - lam) * x[index, :]\n    return mixed_x, y, y[index], lam\n\ndef mix_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1. - lam) * criterion(pred, y_b)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TRAIN FUNCTION \n\ndef train_for_epochs_mix(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, fold, model_save_dir, ema_model=None):\n    best_acc = 0.0\n    scaler = torch.cuda.amp.GradScaler()\n    accumulation = max(1, ACCUMULATION_STEPS)\n\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        running_corrects = 0\n        total = 0\n\n        pbar = tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}/{epochs} [Train]\")\n        for batch_idx, (images, labels, src_flags) in enumerate(pbar):\n            images = images.to(DEVICE); labels = labels.to(DEVICE); src_flags = src_flags.to(DEVICE)\n\n            r = random.random()\n            is_mixed = False\n\n            with torch.cuda.amp.autocast():\n                if r < 0.25:\n                    imgs, y_a, y_b, lam = mixup_data(images, labels, alpha=1.0)\n                    outputs = model(imgs)\n                    loss = mix_criterion(criterion, outputs, y_a, y_b, lam)\n                    is_mixed = True\n                elif r < 0.5:\n                    imgs, y_a, y_b, lam = cutmix_data(images, labels, alpha=1.0)\n                    outputs = model(imgs)\n                    loss = mix_criterion(criterion, outputs, y_a, y_b, lam)\n                    is_mixed = True\n                else:\n                    outputs = model(images)\n                    loss = criterion(outputs, labels)\n\n                if src_flags.sum() > 0:\n                    pseudo_ratio = (src_flags == 1).float().mean().item()\n                    loss = loss * (1.0 - 0.5 * pseudo_ratio)\n\n                loss = loss / accumulation\n\n            scaler.scale(loss).backward()\n\n            if (batch_idx + 1) % accumulation == 0 or (batch_idx + 1) == len(train_loader):\n                try:\n                    scaler.unscale_(optimizer)\n                except Exception:\n                    pass\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n\n                # EMA update if provided\n                if ema_model is not None:\n                    try:\n                        ema_model.update(model)\n                    except:\n                        pass\n\n                try:\n                    scheduler.step(epoch + batch_idx / float(len(train_loader)))\n                except Exception:\n                    try:\n                        scheduler.step()\n                    except Exception:\n                        pass\n\n            with torch.no_grad():\n                preds = torch.argmax(outputs.detach(), dim=1)\n                if not is_mixed:\n                    running_corrects += torch.sum(preds == labels).item()\n                running_loss += loss.item() * images.size(0) * accumulation\n                total += images.size(0)\n\n            pbar.set_postfix_str(f\"loss={running_loss/total:.4f} acc={running_corrects/total:.4f}\")\n\n        train_loss = running_loss / total if total > 0 else 0.0\n        train_acc = running_corrects / total if total > 0 else 0.0\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        val_corrects = 0\n        val_total = 0\n        with torch.no_grad(), torch.cuda.amp.autocast():\n            for images, labels, _ in val_loader:\n                images = images.to(DEVICE); labels = labels.to(DEVICE)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * images.size(0)\n                preds = torch.argmax(outputs, 1)\n                val_corrects += torch.sum(preds == labels).item()\n                val_total += images.size(0)\n\n        val_loss = val_loss / val_total if val_total > 0 else 0.0\n        val_acc = val_corrects / val_total if val_total > 0 else 0.0\n\n        print(f\"[Fold {fold+1}][Epoch {epoch+1}/{epochs}] train_loss={train_loss:.4f} train_acc={train_acc:.4f} | val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n\n        # Save best model\n        if val_acc > best_acc:\n            best_acc = val_acc\n            save_path = os.path.join(model_save_dir, f\"best_model_fold_{fold}.pth\")\n            try:\n                if ema_model is not None:\n                    ema_state = ema_model.module.state_dict() if hasattr(ema_model, \"module\") else ema_model.state_dict()\n                    torch.save(ema_state, save_path)\n                    print(\"Saved EMA weights to\", save_path)\n                else:\n                    torch.save(model.state_dict(), save_path)\n                    print(\"Saved model weights to\", save_path)\n            except Exception as e:\n                print(\"EMA save failed:\", e, \"â€” saving model.state_dict() instead\")\n                torch.save(model.state_dict(), save_path)\n\n    print(f\"Fold {fold+1} finished. Best val_acc={best_acc:.4f}\")\n    return best_acc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Helper: K-Fold run (teacher/student reuse)\n\nfrom sklearn.model_selection import StratifiedKFold\n\ndef run_kfold_train(data_paths, data_labels, model_save_dir, n_folds, epochs, is_teacher=True):\n    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n    for fold, (train_idx, val_idx) in enumerate(skf.split(data_paths, data_labels)):\n        print(f\"\\n=== Fold {fold+1}/{n_folds} ===\")\n        train_paths = [data_paths[i] for i in train_idx]\n        train_labels = [data_labels[i] for i in train_idx]\n        val_paths = [data_paths[i] for i in val_idx]\n        val_labels = [data_labels[i] for i in val_idx]\n\n        train_ds = SimpleImageDataset(train_paths, train_labels, transform=train_tfms, source_flags=[0]*len(train_paths))\n        val_ds = SimpleImageDataset(val_paths, val_labels, transform=val_test_tfms, source_flags=[0]*len(val_paths))\n\n        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n\n        model = get_model(BACKBONE, n_classes, pretrained=True).to(DEVICE)\n\n        # EMA for teacher only (is_teacher)\n        ema_model = None\n        if is_teacher and ModelEmaV2 is not None:\n            try:\n                ema_model = ModelEmaV2(model, decay=0.9999)\n                print(\"ModelEmaV2 enabled for teacher.\")\n            except:\n                ema_model = None\n\n        criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n        # STAGE 1: freeze backbone, train head\n        for param in model.parameters():\n            param.requires_grad = False\n        try:\n            classifier = model.get_classifier()\n            for p in classifier.parameters():\n                p.requires_grad = True\n            head_params = list(classifier.parameters())\n        except Exception:\n            # fallback: unfreeze last 20 params\n            all_params = list(model.parameters())\n            for p in all_params[-20:]:\n                p.requires_grad = True\n            head_params = all_params[-20:]\n        optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=3e-4, weight_decay=1e-2)\n        # use OneCycleLR for head stage for faster convergence\n        steps_per_epoch = max(1, len(train_loader))\n        scheduler = OneCycleLR(optimizer, max_lr=3e-4, epochs=max(1, epochs//2), steps_per_epoch=steps_per_epoch, pct_start=0.2)\n\n        # STAGE 2: unfreeze top layers (~30%)\n        total_params = len(list(model.parameters()))\n        for i, p in enumerate(model.parameters()):\n            p.requires_grad = (i > total_params * 0.7)\n        # parameter-grouping: slightly higher lr for newly-unfrozen params?\n        opt_params = [\n            {'params': [p for p in model.parameters() if p.requires_grad], 'lr': 5e-5}\n        ]\n        optimizer = AdamW(opt_params, lr=5e-5, weight_decay=1e-2)\n        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-6)\n\n        # STAGE 3: full fine-tune\n        for p in model.parameters():\n            p.requires_grad = True\n        optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=1e-2)\n        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-6)\n        \n        train_for_epochs_mix(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs - epochs//2 - epochs//4, fold, model_save_dir, ema_model=ema_model)\n\n        del model, train_loader, val_loader, train_ds, val_ds, ema_model\n        gc.collect()\n        torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Pseudo-label generation\n\ndef load_teacher_models(model_dir, n_folds):\n    models = []\n    for f in range(n_folds):\n        p = os.path.join(model_dir, f\"best_model_fold_{f}.pth\")\n        if os.path.exists(p):\n            m = get_model(BACKBONE, n_classes, pretrained=False).to(DEVICE)\n            m.load_state_dict(torch.load(p, map_location=DEVICE))\n            m.eval()\n            models.append(m)\n            print(\"Loaded teacher fold\", f)\n    return models\n\ndef tta_predict(models_list, img_path):\n    img = Image.open(img_path).convert(\"RGB\")\n    all_probs = []\n    with torch.no_grad():\n        for m in models_list:\n            m.eval()\n            probs = []\n            for tfm in tta_tfms:\n                x = tfm(img).unsqueeze(0).to(DEVICE)\n                out = m(x)\n                probs.append(F.softmax(out, dim=1).cpu().numpy())\n            probs = np.mean(probs, axis=0)  # avg TTA\n            all_probs.append(probs)\n    final = np.mean(all_probs, axis=0)[0]\n    return final  # shape: (n_classes,)\n\ndef generate_pseudo_labels(teacher_models, test_imgs, csv_path, threshold=CONFIDENCE_THRESHOLD):\n    records = []\n    for p in tqdm(test_imgs, desc=\"Pseudo labeling\"):\n        probs = tta_predict(teacher_models, p)\n        idx = int(np.argmax(probs))\n        conf = float(np.max(probs))\n        records.append({\"ID\":os.path.basename(p), \"path\":p, \"predicted_index\":idx, \"confidence\":conf})\n    df = pd.DataFrame(records)\n    df.to_csv(csv_path, index=False)\n    print(\"Pseudo saved:\", csv_path)\n    df_strong = df[df['confidence'] >= threshold].copy()\n    print(\"Pseudo strong count:\", len(df_strong))\n    return df, df_strong","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Student training\n\ndef train_student_with_pseudo(original_paths, original_labels, df_pseudo_strong, student_model_dir, n_folds, epochs):\n    pseudo_paths = df_pseudo_strong['path'].tolist()\n    pseudo_labels = df_pseudo_strong['predicted_index'].astype(int).tolist()\n    combined_paths = original_paths + pseudo_paths\n    combined_labels = original_labels + pseudo_labels\n    source_flags = [0]*len(original_paths) + [1]*len(pseudo_paths)\n    print(\"Student dataset size:\", len(combined_paths), \"pseudo:\", len(pseudo_paths))\n    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n    for fold, (train_idx, val_idx) in enumerate(skf.split(combined_paths, combined_labels)):\n        print(f\"\\n=== Student Fold {fold+1}/{n_folds} ===\")\n        train_paths = [combined_paths[i] for i in train_idx]\n        train_labels = [combined_labels[i] for i in train_idx]\n        train_flags  = [source_flags[i] for i in train_idx]\n\n        val_paths = [combined_paths[i] for i in val_idx]\n        val_labels = [combined_labels[i] for i in val_idx]\n        val_flags  = [source_flags[i] for i in val_idx]\n\n        train_ds = SimpleImageDataset(train_paths, train_labels, transform=train_tfms, source_flags=train_flags)\n        val_ds = SimpleImageDataset(val_paths, val_labels, transform=val_test_tfms, source_flags=val_flags)\n\n        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=2, pin_memory=True)\n\n        model = get_model(BACKBONE, n_classes, pretrained=True).to(DEVICE)\n        criterion = LabelSmoothingCrossEntropy(smoothing=0.05)\n        backbone_params = []\n        head_params = []\n        for name, p in model.named_parameters():\n            if 'head' in name or 'classifier' in name or 'fc' in name:\n                head_params.append(p)\n            else:\n                backbone_params.append(p)\n        \n        param_groups = [\n            {'params': backbone_params, 'lr': 5e-6},\n            {'params': head_params, 'lr': 5e-5}\n        ] if len(head_params) > 0 else [{'params': model.parameters(), 'lr': 5e-5}]\n        \n        optimizer = AdamW(param_groups, weight_decay=1e-2)\n        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1, eta_min=1e-6)\n\n        train_for_epochs_mix(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs, fold, student_model_dir, ema_model=None)\n\n        del model, train_loader, val_loader\n        gc.collect()\n        torch.cuda.empty_cache()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MAIN: execute pipeline\n\n# 1) prepare original dataset info\nfull = datasets.ImageFolder(TRAIN_DIR)\nclass_names = full.classes\nn_classes = len(class_names)\nprint(\"Classes:\", class_names)\n\noriginal_paths = [p for p,_ in full.samples]\noriginal_labels = [int(l) for _,l in full.samples]  # safer: use full.targets if needed\ntest_imgs = sorted(glob(os.path.join(TEST_DIR, \"*\")))\n\n# 2) Train Teacher (K-Fold)\nprint(\"\\n*** START TEACHER TRAINING ***\")\nrun_kfold_train(original_paths, original_labels, MODEL_DIR_TEACHER, N_FOLDS_TEACHER, EPOCHS_TEACHER, is_teacher=True)\n\n# 3) Load teacher models & generate pseudo labels\nteacher_models = load_teacher_models(MODEL_DIR_TEACHER, N_FOLDS_TEACHER)\ndf_pseudo_all, df_pseudo_strong = generate_pseudo_labels(teacher_models, test_imgs, PSEUDO_CSV_PATH, threshold=CONFIDENCE_THRESHOLD)\n\nif len(df_pseudo_strong) < max(50, int(0.05 * len(test_imgs))):\n    thr = max(0.75, df_pseudo_all['confidence'].quantile(0.75))\n    print(\"Adaptive lower threshold to\", thr)\n    df_pseudo_strong = df_pseudo_all[df_pseudo_all['confidence'] >= thr].copy()\n    print(\"New strong count:\", len(df_pseudo_strong))\n\n# 4) Student training\nprint(\"\\n*** START STUDENT TRAINING (real + pseudo) ***\")\ntrain_student_with_pseudo(original_paths, original_labels, df_pseudo_strong, MODEL_DIR_STUDENT, N_FOLDS_STUDENT, EPOCHS_STUDENT)\n\n# 5) Final inference ensemble (Student models)\nprint(\"\\n*** FINAL INFERENCE WITH STUDENT MODELS ***\")\nstudent_models = []\nfor f in range(N_FOLDS_STUDENT):\n    p = os.path.join(MODEL_DIR_STUDENT, f\"best_model_fold_{f}.pth\")\n    if os.path.exists(p):\n        m = get_model(BACKBONE, n_classes, pretrained=False).to(DEVICE)\n        m.load_state_dict(torch.load(p, map_location=DEVICE))\n        m.eval()\n        student_models.append(m)\nprint(\"Loaded student models:\", len(student_models))\n\n# inference\nrows = []\nfor p in tqdm(test_imgs, desc=\"Final Inference\"):\n    probs = tta_predict(student_models, p)\n    idx = int(np.argmax(probs))\n    rows.append({\"ID\": os.path.basename(p), \"label\": class_names[idx]})\n\npd.DataFrame(rows).to_csv(OUT_FILE, index=False)\nprint(\"Submission saved to\", OUT_FILE)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}